 \documentclass[12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}       

\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm,amssymb,amsbsy,amstext,amscd,amsxtra,multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage[stable]{footmisc}
\usepackage{ dsfont }
\usepackage{wrapfig}
\usepackage{xparse}
\usepackage{ifthen}
\usepackage{bm}
\usepackage{color}

\graphicspath{{~/Desktop/opt/Optimization-Halving-The-Square/tests/}}

\usepackage{algorithm}
\usepackage{algpseudocode}
	
\usepackage{xcolor}
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{799B03} % цвет гиперссылок
\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\intt}{int}
\DeclareMathOperator{\conv}{conv}
\begin{document}

\tableofcontents
\newpage

\section{Introduction}

In this paper we research one method of optimization on a square in $\mathbb{R}^2$. The method was offered by Nesterov (see ex.~4 from ~\cite{task}). 

In the paper~\cite{Ston_Pas} there are some results for this method. Namely, there are some estimates for iterations number and accuracy for task on segment (see the next section or \cite{Ston_Pas}). Also there are comparison of this method with method of ellipsoids. Discussed method showed better results on time.

In this paper we want to continue this research. In the next section there is description of method and pseudocode for it. In the section~3. there are results where this method works correctly. The section~4. includes two ways to estimate accuracy for a task on segment and it is one of two main results in this paper. Following important result is theorem 5.3 for iterations number that is asymptotically twice as good as the estimate in \cite{Ston_Pas}.

Section Tests include different tests that show some theoretical results in practice and include comparison of this method with gradient descent. 

\section{Description Of Method}

Let's consider a following task:

$$\min_{(x,y)}\left\{f(x,y)|(x,y) \in Q\right\},$$
where $f$ is a convex function, $Q$ - is a square on the plane.

Let's consider a following method. One solves task of minimization for a function $g(x) = f\left(x, y_0 = \frac{a}{2}\right)$ on a segment $[0, a]$ with an accuracy $\delta$ on function. After that one calculates a sub-gradient in a received point and chooses the rectangle which the sub-gradient "does not look" in. Similar actions are repeated for a vertical segment. As a result we have the square decreased twice. Let's find a possible value of error $\delta_0$ for task on segment and a sufficient iteration's number $N$ to solve the initial task with accuracy $\epsilon$ on function.

Let's describe an algorithm formally. See pseudo-code \ref{alg:Example}.

\begin{algorithm}[H]
\caption{Algorithm of the method}\label{alg:Example}
\begin{algorithmic}[1]
\Function{Method}{convex function $f$, square $Q = [a, b]\times[c,d]$}

$x_0:= solve(g = f(\cdot, \frac{c+d}{2}), [a,b], \delta)$
 
$g = subgradient(f, (x_0, \frac{c+d}{2}))$
 
\If{g[1] > 0}
\State$Q := [a,b] \times [c, \frac{c+d}{2}]$ 
\Else
\State$Q := [a,b] \times [\frac{c+d}{2}, d]$ 
\EndIf

 $y_0:= solve(g = f(\frac{a+b}{2}, \cdot), [c,d], \delta)$
 
 $g := subgradient(f, (\frac{a+b}{2}), y_0)$
 
 \If{g[0] > 0}
 \State$Q := [a, \frac{a+b}{2}] \times [c, d]$ 
\Else
\State$Q := [\frac{a+b}{2},b] \times [c, d]$ 
\EndIf
\If{StopRec() == False}
\State Method($f$, $Q$) 
\EndIf
\Return $(\frac{a+b}{2}, \frac{c+d}{2})$
\EndFunction 
 \end{algorithmic}
\end{algorithm}

\section{Algorithm correctness}
Let's $\textbf{x}_0$ is solution of the task on segment, $Q_1$ is choosed rectangle, $Q_2$ is not choosed rectangle.

\subsection{Zero Error}

\begin{lemma}\label{l1}
If the optimization task on segment is solved with zero error and the $f$ is convex and differentiable at a point-solution, rectangle with solution of initial task was choosed correct.
\end{lemma}
\begin{proof}
From sub-gradient definition, $\textbf{x}^* \in \{x|(\textbf{g}(\textbf{x}_0), \textbf{x}_0 - \textbf{x}^*) \geq 0)\}$. Lemma's statement follows from it and a fact that the first (or the second for vertical segment) gradient's component in point $\textbf{x}_0$ is zero.
\end{proof}


\subsection{Nonzero Error}
\begin{theorem}
Let's the $f$ has continuous derivative on the square. Then there is a neighbourhood of a solution of optimization task on segment such as a choice of rectangle will not change if one use any point from the   neighbourhood.
\end{theorem}
\begin{proof}
Let's consider a case when we work with horizontal segment. Case with vertical segment is considered analogously. Then we are interesting in $f_y'(x_0, y_0)$. If $\textbf{x}_0$ is not solution of initial task, then $f_y'(x_0, y_0) \neq 0$. 

From a continuity of the derivative:

$$\lim\limits_{\delta \rightarrow 0}f_y'(x_0+\delta, y_0) = f_y'(x_0, y_0)$$

Therefore, 

$$\exists \delta_0:\forall \textbf{x}_\delta\in B_{\delta_0}(x_0)\times y_0\Rightarrow \sign(f_y'(\textbf{x}_\delta)) = \sign(f_y'(\textbf{x}_0))$$

From it and lemma 3.1 theorem's statement follows.

\end{proof}

\subsection{Undifferentiable convex function}

The method does not work for all convex functions even for zero error on segment. 

\textbf{Example 1.} There is an example in \cite{Ston_Pas}.

\section{Error's Value}

From derivative continuously we have following obvious result:

\begin{lemma}
If $f$ has continuous derivative. If $|f'_y(x, y_0)| > 0$ for all $x$ on horizontal segment, then the second gradient's component has same sign at all points of segment. If $|f'_x(x_0, y)| > 0$ for all $y$ on vertical segment, then the first gradient's component has same sign at all points of segment.
\end{lemma}

\textbf{Example 2.} All functions $f$ of the following type meet conditions of written above lemma:

$$f(x, y) = \psi(x) + \phi(y),$$
where $\psi, \phi$ are convex and differentiable functions.

\textbf{Example 3.} Let's illustrate that we can not always take any point from segment. Let's consider following task:

$$\min\left\{(x-y)^2 + x^2\Big|Q = [0,1]^2\right\}$$

On segment $[0,1]\times\left\{\frac{1}{2}\right\}$ this task has solution $f^* = f\left(\frac{1}{4}, \frac{1}{2}\right) = \frac{1}{8}$. Derivative on $y$ at this point is $f'_y\left(\frac{1}{4}, \frac{1}{2}\right) = \frac{]}{2}$ but at the point $\left(1, \frac{1}{2}\right)$ is equal to $-1$. We can see that in this case rectangle will selected non-correctly.

Let's find possible value of error's value, i.e. let's find a number $\delta_0$ such as if an error for   a solution on a segment is less $\delta_0$ then rectangle for the segment is defined correctly. Rectangles are defined correctly for a horizontal optimization task, if:

\begin{equation}\label{MCH}
\forall\delta : |\delta| < \delta_0 \Rightarrow f'_y(\textbf{x}_0)f'_y(x_0+\delta, y_0) > 0
\end{equation}

Analogically, for a vertical segment:
\begin{equation}\label{MCV}
\forall\delta: |\delta| < \delta_0 \Rightarrow f'_x(\textbf{x}_0)f'_x(x_0, y_0+\delta) > 0
\end{equation}




\begin{theorem}
Let function $f$ be convex and has $L$-Lipschitz continuous gradient and a point $\textbf{x}_0$ is a solution of optimization's  task on a current segment. 

The current segment is horizontal and $\exists M>0  : \Rightarrow|f_y'(\textbf{x}_0)| \geq M$ or the current segment is vertical and $\exists M>0  : \Rightarrow|f_x'(\textbf{x}_0)| \geq M$. Then rectangle is defined correctly if the possible value of error is less than $\frac{M}{L}$.
\end{theorem}
\begin{proof}
Condition \eqref{MCH} is met if there is a derivative $f'_y(x_0+\delta, y_0)$ in a neighbourhood of 
$f'_y(\textbf{x}_0)$ with radius $\left|f'_y(\textbf{x}_0)\right|$:

$$\left|f'_y(\textbf{x}_0) - f'_y(x_0+\delta, y_0)\right|<\left|f'_y(\textbf{x}_0)\right|$$

The $L$-Lipschitz continuity gives following inequality:

$$\left|f'_y(\textbf{x}_0) - f'_y(x_0+\delta, y_0)\right| \leq L|\delta|$$

Therefore the following possible value is sufficient to select rectangle correctly:

$$\delta_0 < \frac{M}{L} \leq \frac{\left|f'_y(\textbf{x}_0)\right|}{L}$$

Statement for vertical segment is proved similarly.
\end{proof}

\textbf{Example 4.} All positive semidefinite quadratic form meet conditions of written above theorem:

$$B(x,y) = Ax^2 + 2Bxy + Cy^2 + Dx +Ey + F,$$
$$L = B''_{xy} = B''_{yx} = 2B < \infty$$
where $A \geq 0,AC - B^2\geq0$. Also estimate for $\delta$ has some sense (estimate is non zero if $\textbf{x}_0$ is not optimal solution). Also one can easy show that this estimate is accurate for task from example 3.

Theorem 4.1 gives conditions of stop in the case when gradient in point-solution on segment is large. But what should we do if gradient in this point is small?

\begin{theorem}
Let $f$ be convex and has $L$-Lipschitz continuous gradient. Then for accuracy on function $\epsilon$ following condition in point $\textbf{x}$ is sufficient:

$$\|\nabla f(\textbf{x})\|\leq \frac{\epsilon}{a\sqrt{2}}, $$
where $a$ is size of current square.
\end{theorem}

\begin{proof}
Let's consider following inequality for convex functions (see prove in \cite{Nesterov}):

$$f(\textbf{x}^*) - f(\textbf{x}) \geq (\nabla f(\textbf{x}), \textbf{x}^* - \textbf{x} )$$

Using  Cauchy–Bunyakovsky–Schwarz inequality one has following inequality

$$f(\textbf{x}) - f(\textbf{x}^*) \leq -(\nabla f(\textbf{x}), \textbf{x}^* - \textbf{x} )\leq$$
$$\leq \|\nabla f(\textbf{x})\| \|\textbf{x}^* - \textbf{x}\|\leq \|\nabla f(\textbf{x})\|a\sqrt{2}$$

This inequality proves theorem's statement.
\end{proof}

Now let's give a new condition to stop optimization on segment.

\begin{theorem}
Let $f$ be convex and has $L$-Lipschitz continuous gradient on horizontal segment.

If following condition is met 
$$|f'_y(x_0+\delta, y_0)|^2 \geq L^2\delta_0^2 - (f'_x(x_0 + \delta, y_0))^2, \text{where } \delta_0\geq|\delta| $$
and solution on the segment $\textbf{x}_0$ is internal point
then 
$$\sign\left(f'_y(x_0, y_0)\right) = \sign\left(f'_y(x_0+\delta, y_0) \pm \sqrt{L^2\delta^2 - (f'_x(x_0 + \delta, y_0))^2}\right)$$
\end{theorem}
\begin{proof}
The $f$ has $L$-Lipschitz continuous gradient. Therefore,

$$\left((f'_x(x_0 + \delta, y_0))^2 + \left(f'_y(x_0+\delta, y_0) - f'_y(\textbf{x}_0)\right)^2\right)^\frac{1}{2}\leq L\delta_0$$

$$|f'_y(x_0+\delta, y_0) - f'_y(\textbf{x}_0)|\leq \sqrt{L^2\delta_0^2 - (f'_x(x_0 + \delta, y_0))^2} = q$$
Then, if
$$(f'_y(x_0+\delta, y_0)+q)(f'_y(x_0+\delta, y_0)-q) = (f'_y(x_0+\delta, y_0))^2-q^2 \geq 0$$
then we can determine sign of $f'_y(\textbf{x}_0)$ accurately. Written above inequality is equivalent to inequality from theorem's condition.
\end{proof}

Let's make a couple of remarks.

Firstly, theorem 4.1 needs some lower bound for $f'_{x}(\textbf{x}_0)$ and $f'_{y}(\textbf{x}_0)$ on segments and it can be hard task.

Secondly, we can replace the Lipschitz condition on the square by the Lipschitz condition on the segments in the theorem 4.1.

Thirdly, all theorems in this section use $f'_y$ on horizontal segment and $f'_x$ on vertical segment. As a result, if one checks condition of stop on each iteration method will use full gradient on each iteration. It can slow down method but this estimates can be better than in ~\cite{Ston_Pas}. As a result, method can work faster.

\section{Number of iterations}

Following estimates are correct if each iterations was correct (a rectangle is selected correctly on each iterations).

\begin{theorem}
If function $f$ is convex and $L_f$-Lipschitz continuous, then for to solve initial task with accuracy $\epsilon$ on function one has to take a center of a current square as approximate  solution and make following iteration's numbers:
\begin{equation}\label{NI1}N = \left\lceil\log_2\frac{L_fa}{\sqrt{2}\epsilon}\right\rceil\end{equation}
where $a$ is a size of the initial square $Q$.
\end{theorem}
This estimate is a little improved estimate from \cite{Ston_Pas} because of we use a center of a current square as approximate  solution. The prove is similar to prove of not improved estimate.

There are functions which estimates from written above theorem are very accurate for.

\textbf{Example 5.} Let's consider following task with positive constant $A$:

$$\min\left\{A(x+y)|Q = [0,1]^2\right\}$$

If one take a center of a current solution as approximate  solution one have value $\frac{A}{2^N}$ after $N$ iterations. Therefore, for accuracy $\epsilon$ one has to $\lceil\log_2\frac{A}{\epsilon}\rceil$. For this function $L_f = 2A$. Therefore, estimate \eqref{NI1} is accurate for such tasks with little error that not more one iteration.

But any convex function is locally Lipschitz continuous at all $x \in \intt Q$. Therefore, we have following theorem.

\begin{theorem}
If function $f$ is convex and a solution $x^*\in \intt Q$, then for to solve initial task with accuracy $\epsilon$ on function one has to take a center of a current square as approximate  solution and make following iteration's numbers:
\begin{equation}\label{NI2}
N = \left\lceil\log_2\max\left\{\frac{a}{\epsilon_0(\textbf{x}^*)},\frac{L_fa}{\sqrt{2}\epsilon}\right\}\right\rceil
\end{equation}
where $a$ is a size of the initial square $Q$, $\epsilon_0(x^*)$ is size of neighbourhood of $x^*$ which $f$ is $L_f$-Lipschitz continuous in, $\Delta f =  f(x_0) - f(x^*)$, $x_0$ is a center of square $Q$.
\end{theorem}

We can improve written above estimates if to add new conditions:

\begin{theorem}
Let function $f$ be convex and has $L$-Lipschitz continuous gradient.

If solution is a internal point, then for to solve initial task with accuracy $\epsilon$ on function one has to take a center of a current square as approximate  solution and make following iteration's numbers:
\begin{equation}\label{NI3}N = \left\lceil\frac{1}{2}\log_2\frac{La^2}{4\epsilon}\right\rceil\end{equation}
where $a$ is a size of the initial square $Q$.
\end{theorem}

\begin{proof}
For all convex functions there is following inequality (one may find proof in \cite{Nesterov}):
$$f(\textbf{x}) - f(\textbf{x}^*) - (f'(\textbf{x}^*), \textbf{x} - \textbf{x}^*) \leq \frac{L}{2}\|\textbf{x}-\textbf{x}^*\|^2$$

If $\textbf{x}^*$ is a solution and an internal point, then $f'(\textbf{x}^*) = 0$:

$$f(\textbf{x}) - f(\textbf{x}^*)\leq \frac{L}{2}\|\textbf{x}-\textbf{x}^*\|^2$$

After $N$ iterations we have the estimate:

$$f(\textbf{x}) - f(\textbf{x}^*)\leq \frac{L}{4}\left(\frac{a}{2^N}\right)^2$$

Using it we have estimate \eqref{NI3}.
\end{proof}

\section{Tests}

In this section we show estimate on number iterations of practice and compare work time of gradient descent and our new method - halving square\footnote{You can find all code in the repository~\cite{my_git}}. All code was made in Anaconda 5.3.1 Python 3.6 (see cite~\cite{conda})

\subsection{About function $solve$}

If you look at pseudocode~\ref{alg:Example} you can find function $solve$. This function solves task of minimization on segment. A cost of one iteration depends on a choice of its implementation. In our tests we will use gradient descent with step $h_k = \frac{h}{\sqrt{k}} = \frac{a}{4\sqrt{k}}$, where $a$ is size of current segment.

\subsection{Tests for iterations number}

Let's make tests for the estimate \eqref{NI1}.

Functions $-\sin\frac{\pi x}{a}$ and $-\sin\frac{\pi x}{b}$ are convex on square $Q = [0,1]^2$ when $a,b\geq 1$. Therefore, a function $-A\sin\frac{\pi x}{a} - B\sin\frac{\pi y}{b}$ is convex for all $A,B\geq 0$ as cone combination of convex function.

Functions $x^n$ are convex and monotonously non-decreasing on $[0, 1]$ for all $n \in \mathbb{N}$ that's why functions $\left(-A\sin\frac{\pi x}{a} - B\sin\frac{\pi y}{b} + A + B + D\right)^n$ are convex for all $D\geq 0$.

Therefore, following function is convex:

$$f(x,y) = -A_1\sin\frac{\pi x}{a_1} - B_1\sin\frac{\pi x}{b_1} + \sum\limits_{n=2}^N\left(-A_n\sin\frac{\pi x}{a_n} - B_n\sin\frac{\pi y}{b_n} + A_n + B_n + D_n\right)^n,$$

where $A_i, B_i. D_i\geq 0$ and $a_i, b_i \geq 1$ for all $i = \overline{1, n}$.

The function $f$ is differentiable infinite times and we can use it to test the method.

Let's take $a_1 = \dots = a_n = a$ and $b_1 = \dots = b_n = b$:

$$f(x,y) = -A_1\sin\frac{\pi x}{a} - B_1\sin\frac{\pi x}{b} + \sum\limits_{n=2}^N\left(-A_n\sin\frac{\pi x}{a} - B_n\sin\frac{\pi y}{b} + A_n + B_n + D_n\right)^n,$$

where $A_i, B_i. D_i\geq 0$ for all $i = \overline{1, n}$ and $a, b \geq 1$.

We have following estimates for derivatives on horizontal and vertical segments:

$$|f'_x|\Big|_{x = x_0} \geq \left(A_1 + \sum\limits_{n=2}^N n A_n D_n^{n-1}\right)\frac{\pi}{a}\Big|\cos \frac{\pi x_0}{a}\Big|$$
$$|f'_y|\Big|_{y = y_0} \geq \left(B_1 + \sum\limits_{n=2}^N n B_n D_n^{n-1}\right)\frac{\pi}{b}\Big|\cos \frac{\pi y_0}{b}\Big|$$

Therefore this functions met conditions of lemma 3.1 and we can take gradient at any point of segment. One can see examples of functions $f$ on fig.~\ref{fig:examples}

\begin{figure}[H]
\center{\includegraphics[scale=0.6]{tests/Examples.png}}
\caption{Examples of tests functions}
\label{fig:examples}
\end{figure}

We will use $a$ and $b$ from $[1, 2]$ and $N = 2$. Let's solve task of minimization function $f$ with different parameters on square $[0,1]^2$ through new method and compares number of iteration with estimate~\eqref{NI1}.

Result of the test you can see on fig.~\ref{fig:image}.

\begin{figure}[h!]
\center{\includegraphics[scale=0.7]{tests/Test_results.png}}
\caption{Tests' results for iterations number}
\label{fig:image}
\end{figure}

On graphic $N$ is number of done iterations for accuracy $\epsilon$, $\log_2\frac{La}{\sqrt{2}\epsilon}$ is the parameter of tests tasks. Line $N = \log_2\frac{La}{\sqrt{2}\epsilon}$ is our estimates \eqref{NI1}.

We can see that there are tests points under this line. It confirms our estimates \eqref{NI1}.

\subsection{Comparison}

Let's compare experimentally work's time of gradient descent and method of halving square. We will compare it on optimization task for functions from previous subsection and random quadric function with positive semidifenite matrix:
\begin{equation}\label{ex_1}
f(x,y) = -A_1\sin\frac{\pi x}{a} - B_1\sin\frac{\pi x}{b} + \left(-A_2\sin\frac{\pi x}{a} - B_2\sin\frac{\pi y}{b} + A_2 + B_2 + D_2\right)^2,
\end{equation}
where $A_i, B_i. D_i\geq 0$ for all $i = \overline{1, 2}$ and $a, b \in [1,2]$.

\begin{equation}\label{ex_2}
f(x,y) = (Ax + By)^2 + Cx^2 + Dx + Ey+ F, C\geq 0
\end{equation}
Halving square will stop when current value will be no further than $\epsilon$ from optimal value. Gradient descent will stop when distance between current and previous value will be not more than $\epsilon$.

We will use gradient descent with a step $h_k = \frac{h}{\sqrt{k}} = \frac{a}{4\sqrt{k}}$, where $a$ is size of the square. Also we add limit $N_{max} = 1000$ on the iterations number for method of halving square, solving on segment and gradient descent.

When we compare two method we distinguish the following situations (similar name of situations on graphics):

\begin{itemize}
\item Type 1 (T1) - tasks which gradient descent completed but halving square was not complete,
\item Type 2 (T2) - gradient descent is faster than halving square,
\item Type 3 (T3) - work's times are approximately equal or time is too short to measure,
\item Type 4 (T4) - gradient descent is slower than halving square,
\item Type 5 (T5) - gradient descent was not complete but halving square completed,
\item Type 6 (T6) - both methods were not completed successfully
\end{itemize}


\subsubsection{Sinuses}  

As already mentioned in previous subsection, functions \eqref{ex_1} meet conditions of lemma 3.1, therefore, we can take any point from segment on each iteration in method of halving square. Using all written above we have following results (see fig.~\ref{comp_sinuses}).

\begin{figure}[h!]
\center{\includegraphics[scale=0.6]{tests/Comparison-sinuses.png}}
\caption{Tests' results for comparison on sinuses}
\label{comp_sinuses}
\end{figure}

We can see that new method is not less efficient on the most tasks and more efficient on the task's half than gradient descent.

\subsubsection{Quadric functions}

Quadric functions do not always meet conditions of lemma 3.1, therefore, we can not think that error for task on segment can be very big. We can try to estimate this error but let's use for this task gradient descent with $|x-x_{prev}|<\delta$, where $\delta$ is much less than size of segment. In our experiments we will use $\delta = \frac{a}{400}$. Using it we have following results (see fig.~\ref{comp_pol}).

\begin{figure}[H]
\center{\includegraphics[scale=0.6]{./tests/Comparison-pol.png}}
\caption{Tests' results for comparison on quadric function}
\label{comp_pol}
\end{figure}

We can see that halving square was not completed successfully on big parts of task - about $\frac{1}{6}$ from all tasks. We can decrease $\delta$ in its modification, but work's time increases in this case.

Also gradient descent was better than halving square in more than half tests. Increasing $\delta$ lead to 
decreasing work's time but also it can lead to that method will not completed successfully more often.

Therefore, halving square with proposed function $solve$ and proposed stupid strategy for $\delta$ is not efficient.

\section{Conclusion}
\newpage
\begin{thebibliography}{3}
\bibitem{task}
Gasnikov A.  Universal gradient descent // MIPT --- 2018, 240 p.
\bibitem{Ston_Pas}
Pasechnyk D.A., Stonyakin F.S.  One method for minimization a convex Lipchitz continuous function of two variables on a fixed square // arXiv.org e-Print archive. 2018. – URL: \href{https://arxiv.org/pdf/1812.10300.pdf}{https://arxiv.org/pdf/1812.10300.pdf}
\bibitem{Nesterov}
Nesterov U.E.  Methods of convex optimization // M.MCNMO --- 2010, 262 p.
\bibitem{conda}
Anaconda[site]. At available: \href{https://www.anaconda.com}{https://www.anaconda.com}
\bibitem{my_git}
Repository with code: \href{https://github.com/ASEDOS999/Optimization-Halving-The-Square}{https://github.com/ASEDOS999/Optimization-Halving-The-Square}
\end{thebibliography}
\end{document}
